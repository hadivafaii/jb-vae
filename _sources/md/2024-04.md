Summary
===============================

Summary of progress in April 2024

- weight norm (wn) hurts. do weight reg instead
- when weight reg lamb = 0.01 (too strong): it hurts
- better to anneal lamb as well, start weak and increase to 0.01
- final verdict on lamb: just get rid of it. no need. lamb=0.0
- too large learning rate (0.005) hurts
- init fc_enc matters (currently normal_ with scale=0.05)
- architecture absolutely needs residual connections throughout
    - a key innovation was ResConvPool() which combines AvgPool with a res Conv
- KNN analysis shows log_dr for poisson is more separable than mu for gaussian
- MEI did not work very well. don't waste too much time on it
- CIFAR10-PATCHES and DOVES both give Gabors:
    - DOVES look much better (a variety of spatial frequencies)
    - Both: kl_diag is highly predictive of axial/oblique category
- BALLS disentanglement did not work. no time for further exploration. discard